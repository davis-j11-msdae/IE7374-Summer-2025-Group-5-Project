# Personalized Storytelling System

An AI-powered storytelling system that fine-tunes Mixtral 8x7B to generate age-appropriate stories with user history integration and comprehensive evaluation.

## ğŸ¯ Project Overview

This system creates personalized stories for users of different ages by:
- Fine-tuning Mixtral 8x7B on children's and sci-fi story datasets from Project Gutenberg
- Generating age-appropriate content for 4 age groups: child (0-5), kid (6-12), teen (13-17), adult (18+)
- Maintaining user story history with automatic summarization
- Evaluating story quality, safety, and age appropriateness
- Supporting story continuation and interactive sessions

## ğŸ“ Project Structure

```
IE7374-Summer-2025-Group-5-Project/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ model_config.yaml          # Model and training configuration
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ helpers.py                 # Common utility functions
â”‚   â”œâ”€â”€ environment_check.py       # System validation
â”‚   â”œâ”€â”€ generate_users.py          # User account generator
â”‚   â”œâ”€â”€ download_data.py           # Data and model downloading
â”‚   â”œâ”€â”€ data_loader.py             # Raw data processing
â”‚   â”œâ”€â”€ data_tokenizer.py          # Dataset tokenization
â”‚   â”œâ”€â”€ eval.py                    # Story evaluation and classification
â”‚   â”œâ”€â”€ train.py                   # Model fine-tuning
â”‚   â”œâ”€â”€ hyperparameter_tuning.py   # Optimization of Model Hyperparameters
â”‚   â”œâ”€â”€ history.py                 # User history management
â”‚   â”œâ”€â”€ model_runner.py            # Story generation with authentication
â”‚   â””â”€â”€ samples.py                 # Sample evaluation pipeline
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Downloaded datasets
â”‚   â”œâ”€â”€ processed/                 # Cleaned and age-grouped stories
â”‚   â”œâ”€â”€ tokenized/                 # Tokenized datasets for training
â”‚   â”œâ”€â”€ evaluated/                 # Quality-assessed stories
â”‚   â””â”€â”€ users/                     # User authentication data
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mixtral-7b-base/           # Base Mixtral model
â”‚   â””â”€â”€ tuned_story_llm/           # Fine-tuned storytelling model
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ user_history/              # Individual user story histories
â”‚   â””â”€â”€ samples/                   # Sample evaluation results
â”œâ”€â”€ logs/                          # System logs
â”œâ”€â”€ src/
â”‚   â””â”€â”€ Full.py                    # Main control interface
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <repository-url>
cd IE7574-Summer-2025-Group-5-Project

# Install dependencies
pip install -r requirements.txt

# Create environment file
cat > .env << EOF
OPENAI_API_KEY=your_openai_api_key
HF_TOKEN=your_huggingface_token
KAGGLE_USERNAME=your_kaggle_username
KAGGLE_KEY=your_kaggle_key
EOF
```

### 2. Run the System

```bash
# Navigate to src directory and run main control script
cd src
python Full.py
```

### 3. System Workflow

**Admin Workflow** (requires admin login):
1. Check Environment
2. Download Data and Models  
3. Generate Users File
4. Process and Evaluate Data
5. Tokenize Data
6. Train Model
7. Process Sample Stories
8. Interactive Story Creation

**User Workflow** (regular user login):
- Direct access to Interactive Story Creation

## ğŸ”§ Configuration

### Model Configuration (`configs/model_config.yaml`)

Key settings:
- **Model**: Mixtral 8x7B base model from Hugging Face
- **Training**: 3 epochs, batch size 4, learning rate 2e-5, DeepSpeed optimization
- **Age Groups**: Child (0-5), Kid (6-12), Teen (13-17), Adult (18+)
- **Data Sources**: Project Gutenberg categories with configurable minimum downloads
- **Evaluation**: Perplexity, grammar, coherence, toxicity, and readability metrics

### DeepSpeed Configuration (`configs/deepspeed_config.json`)

Optimized for A100 GPU training:
- ZeRO Stage 2 optimization
- CPU optimizer offloading
- FP16 mixed precision
- Gradient accumulation and clipping

## ğŸ“Š Data Pipeline

### 1. Data Download (`utils/download_data.py`)
- **Project Gutenberg Stories**: Automated download from multiple categories
  - Science Fiction & Fantasy
  - Children's & Young Adult Literature  
  - Adventure Stories
  - Fairy Tales & Folk Tales
  - Mythology & Folklore
  - Humor and Short Stories
- **Base Model**: Mixtral 8x7B from Hugging Face Hub
- **Copyright Validation**: Automatic public domain verification

### 2. Data Processing (`utils/data_loader.py`)
- Extract individual stories from downloaded collections
- Clean and filter content for quality
- Assign age groups based on source categories
- **Integrated Evaluation**: Quality, safety, and appropriateness checking
- Generate comprehensive statistics and summaries

### 3. Tokenization (`utils/data_tokenizer.py`)
- Format stories with age-appropriate instructions
- Tokenize using Mixtral tokenizer with padding
- Create train/validation/test splits (80/10/10)
- Generate tokenization statistics and validation

### 4. Model Training (`utils/train.py`)
- Fine-tune Mixtral 8x7B on processed story datasets
- DeepSpeed optimization for memory efficiency
- Automatic model saving and evaluation metrics
- Comprehensive training statistics and validation

## ğŸ­ Story Generation Features

### User Authentication
- **Admin Users**: Full system access including training and configuration
- **Regular Users**: Story creation and history management
- **Pre-generated Users**: 20 users across age groups (username format: `child_1`, `kid_1`, etc.)
- **Default Credentials**: Password `test` for regular users, `admin` for admin user

### Story Generation
- **Age-Appropriate Content**: Automatic adjustment based on user age
- **History Integration**: Stories can reference previous narratives
- **Story Continuation**: Seamlessly extend existing stories
- **Quality Assurance**: Automatic filtering of inappropriate content
- **Real-time Evaluation**: Grammar, coherence, and safety scoring

### Interactive Features
- User login with age verification
- Create new stories or continue existing ones
- View and manage personal story history with summaries
- Save stories with suggested or custom titles
- Delete unwanted stories from history

## ğŸ“ˆ Evaluation and Quality Control

### Content Filtering
- **Toxicity Detection**: Automatic filtering using Detoxify
- **Age Verification**: Stories matched to appropriate age groups using Flesch-Kincaid scoring
- **Title Validation**: User-provided titles checked for appropriateness
- **Retry Logic**: Automatic regeneration if content fails quality checks

### Quality Metrics
- **Perplexity**: Measured using GPT-2 for text naturalness
- **Grammar/Coherence**: Scored via OpenAI GPT-3.5-turbo API
- **Readability**: Flesch-Kincaid grade level analysis
- **Safety**: Comprehensive toxicity detection across multiple categories
- **Length Validation**: Stories within appropriate word count limits

## ğŸ’¾ User History Management

### Features (`utils/history.py`)
- **Automatic Summarization**: BART-large-CNN for concise story summaries
- **Title Management**: AI-suggested titles with custom override option
- **Story Continuation**: Two modes - update original or save as new story
- **Statistics Tracking**: Word counts, creation dates, reading analytics
- **Storage Optimization**: Maximum 5 stories per user with automatic cleanup

### Storage Format
- Individual JSON files per user in `outputs/user_history/`
- Rich metadata including prompts, evaluations, timestamps
- Cross-session persistence with data validation

## ğŸ–¥ï¸ Hardware Requirements

### Training Requirements
- **GPU**: A100 with 40GB+ VRAM (required for Mixtral 8x7B)
- **RAM**: 32GB+ system memory
- **Storage**: 100GB+ free space for models and data
- **CUDA**: Compatible GPU drivers and toolkit

### Inference Requirements  
- **GPU**: 16GB+ VRAM (RTX 4090, A6000, or equivalent)
- **RAM**: 16GB+ system memory
- **Storage**: 60GB+ for base and fine-tuned models

## ğŸ” System Monitoring

### Status Tracking
- Real-time operation status with timestamps
- Progress bars for long-running operations (>1 minute)
- Comprehensive error reporting with context
- Performance metrics and resource usage monitoring

### Cache Management
- Automatic detection of existing processed data
- User prompts for cache overwrite decisions
- Efficient reuse of previous work and computations
- Storage optimization and cleanup utilities

## ğŸ§ª Sample Stories and Testing

### Automated Testing (`utils/samples.py`)
- 10 comprehensive test prompts across all age groups
- Story continuation testing with context preservation
- Quality and safety validation for all generated content
- Detailed evaluation reports with metrics and statistics

### Sample Categories
- **2 prompts each** for child, kid, teen, and adult age groups
- **2 continuation prompts** demonstrating history integration
- **Comprehensive evaluation** including grammar, coherence, and safety
- **Results archived** in `outputs/samples/` with timestamps

## ğŸ“š Usage Examples

### Complete System Setup
```bash
cd src
python Full.py
# Login as admin (username: admin, password: admin)
# Follow workflow: 1 â†’ 2 â†’ 8 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7
```

### User Story Creation
```bash
cd src  
python Full.py
# Login as regular user (e.g., username: child_1, password: test)
# Create and manage personal stories
```

### Direct Component Access
```bash
# Check system environment
cd utils && python environment_check.py

# Generate user accounts  
cd utils && python generate_users.py

# Download data only
cd utils && python download_data.py
```

## ğŸ”§ Troubleshooting

### Common Issues
1. **CUDA Out of Memory**: Use DeepSpeed config, reduce batch sizes
2. **Download Failures**: Verify internet connection and API credentials
3. **Model Loading Errors**: Ensure complete model downloads and sufficient disk space
4. **OpenAI API Issues**: Verify API key validity and rate limits

### Performance Optimization
- Enable DeepSpeed for memory-efficient training
- Use gradient checkpointing to reduce memory usage
- Optimize batch sizes for available hardware
- Leverage FP16 precision when supported

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch with descriptive name
3. Add comprehensive tests for new functionality
4. Ensure all existing tests and evaluations pass
5. Submit pull request with detailed description

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Mistral AI** for the Mixtral 8x7B foundation model
- **Hugging Face** for transformers library and model hosting
- **Project Gutenberg** for public domain literature datasets
- **OpenAI** for evaluation and quality assessment APIs
- **Microsoft** for DeepSpeed optimization framework
- **Meta** for BART summarization model

---

*Built for educational research in AI-powered creative storytelling*
