# Model Configuration
model:
  name: "mixtral-8x7b-storytelling"
  base_model: "mistralai/Mixtral-8x7B-Instruct-v0.1"
  max_length: 2048
  temperature: 0.7
  top_p: 0.9
  top_k: 50

# Training Configuration - Optimized for VRAM utilization
training:
  batch_size: 12                    # Increased from 4 to utilize VRAM
  learning_rate: 5e-5              # Higher LR for larger batches
  epochs: 5                        # Increased as requested
  gradient_accumulation_steps: 3    # Reduced to maintain effective batch size
  warmup_steps: 50                 # Reduced proportionally
  weight_decay: 0.01
  max_grad_norm: 1.0
  max_train_samples: 8000          # Moderate dataset reduction for speed
  max_eval_samples: 800            # Smaller eval set

# Quantization Configuration - 4-bit for memory efficiency
quantization:
  enabled: true
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA Configuration - Higher rank to utilize available VRAM
lora:
  enabled: true
  task_type: "CAUSAL_LM"
  r: 32                            # Increased from 16
  lora_alpha: 64                   # Scaled with rank
  lora_dropout: 0.1
  target_modules:
    - "q_proj"
    - "k_proj" 
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Data Configuration - Longer sequences to utilize VRAM
data:
  max_sequence_length: 1536        # Increased from 1024
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  min_story_length: 100
  max_story_length: 8000           # Allow longer stories

# Project Gutenberg Configuration
gutenberg:
  min_downloads: 1000

  categories:
    science_fiction_fantasy:
      bookshelf_id: 638
      description: "Science Fiction and Fantasy"

    children_young_adult:
      bookshelf_id: 636
      description: "Children and Young Adult Literature"

    adventure:
      bookshelf_id: 644
      description: "Adventure Stories"

    mythology_legends_folklore:
      bookshelf_id: 646
      description: "Mythology, Legends and Folklore"

    humor:
      bookshelf_id: 641
      description: "Humor"

    short_stories:
      bookshelf_id: 634
      description: "Short Stories"

    fairy_tales:
      bookshelf_id: 216
      description: "Children's Myth, Fairy Tales, etc."

# Age Groups
age_groups:
  child: [0, 5]
  kid: [6, 12]
  teen: [13, 17]
  adult: [18, 100]

# Evaluation Configuration
evaluation:
  perplexity_buckets: [20, 50, 100]
  flesch_kincaid_ranges:
    child: [0, 5.9]
    kid: [6.0, 12.9]
    teen: [13.0, 17.9]
    adult: [18.0, 100]
  grammar_model: "gpt-3.5-turbo"
  coherence_model: "gpt-3.5-turbo"
  toxicity_threshold: 0.5

# History Configuration
history:
  max_history_length: 5
  summary_model: "facebook/bart-large-cnn"
  max_summary_length: 150
  title_max_length: 50

# Paths
paths:
  data_root: "data"
  data_raw: "data/raw"
  data_processed: "data/processed"
  data_tokenized: "data/tokenized"
  data_evaluated: "data/evaluated"
  models: "models"
  outputs: "outputs"
  user_history: "outputs/user_history"
  samples: "outputs/samples"
  users: "data/users"

# Hyperparameter Tuning Configuration
hyperparameter_tuning:
  enabled: true
  sample_percentage: 5  # 5% stratified sample for tuning
  
  search_spaces:
    learning_rate: [2e-5, 5e-5, 1e-4]
    
    lora_rank_alpha:
      - r: 8
        alpha: 16
      - r: 16  
        alpha: 32
      - r: 24
        alpha: 48
    
    batch_size: [4, 8, 12]  # 4 and 12 for VRAM comparison, 8 as middle
    lora_dropout: [0.05, 0.1, 0.2]
    weight_decay: [0.005, 0.01, 0.02]
    warmup_steps: [25, 50, 100]

# DeepSpeed Configuration - Optimized for less CPU offloading
deepspeed:
  enabled: true
  config_path: "configs/deepspeed_config_optimized.json"